# 1. Update your database as the structure changes

Well done so far. You now have a database consisting of five different tables. Now it's time to migrate the data.

# 2. The current database model

Here's the current entity-relationship diagram, showing the five tables.

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/7bf4483c-644d-433c-b5da-e234b331aa4f)

# 3. The current database model

At this moment, only the "university_professors" table holds data. The other four, shown in red, are still empty. In the remainder of this chapter, you will migrate data from the green part of this diagram to the red part, moving the respective entity types to their appropriate tables. In the end, you'll be able to delete the "university_professors" table.

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/c6fa9341-8291-42bc-9acf-2a1adc6429f1)

# 4. Only store DISTINCT data in the new tables

One advantage of splitting up "university_professors" into several tables is the reduced redundancy. As of now, "university_professors" holds 1377 entries. However, there are only 1287 distinct organizations, as this query shows. Therefore, you only need to store 1287 distinct organizations in the new "organizations" table.

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/a0d1a829-347e-4f4c-ad9e-5f5c97174c9d)

# 5. INSERT DISTINCT records INTO the new tables

In order to copy data from an existing table to a new one, you can use the "INSERT INTO SELECT DISTINCT" pattern. After "INSERT INTO", you specify the name of the target table – "organizations" in this case. Then you select the columns that should be copied over from the source table – "unviversity_professors" in this case. You use the "DISTINCT" keyword to only copy over distinct organizations. As the output shows, only 1287 records are inserted into the "organizations" table. If you just used "INSERT INTO SELECT", without the "DISTINCT" keyword, duplicate records would be copied over as well. In the following exercises, you will migrate your data to the four new tables.

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/4d23434c-6d4d-49e9-a648-e08521ddb74e)

# 6. The INSERT INTO statement

By the way, this is the normal use case for "INSERT INTO" – where you insert values manually. "INSERT INTO" is followed by the table name and an optional list of columns which should be filled with data. Then follows the "VALUES" keyword and the actual values you want to insert.

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/f821c915-83c3-4c9e-a76a-70148418206a)

# 7. RENAME a COLUMN in affiliations

Before you start migrating the table, you need to fix some stuff! In the last lesson, I created the "affiliations" table for you. Unfortunately, I made a mistake in this process. Can you spot it? The way the "organisation" column is spelled is not consistent with the American-style spelling of this table, using an "s" instead of a "z". In the first exercise after the video, you will correct this with the known "ALTER TABLE" syntax. You do this with the RENAME COLUMN command by specifying the old column name first and then the new column name, i.e., "RENAME COLUMN old_name TO new_name".

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/6a3bb8b6-40f6-4ae4-baec-7f4c5183e48b)

# 8. DROP a COLUMN in affiliations

Also, the "university_shortname" column is not even needed here. So I want you to delete it. The syntax for this is again very simple, you use a "DROP COLUMN" command followed by the name of the column. Dropping columns is straightforward when the tables are still empty, so it's not too late to fix this error. But why is it an error in the first place?

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/3332724c-609f-4d42-b661-7fa34c870527)

# 9. A professor is uniquely identified by firstname, lastname only

Well, I queried the "university_professors" table and saw that there are 551 unique combinations of first names, last names, and associated universities. I then queried the table again and only looked for unique combinations of first and last names. Turns out, this is also 551 records. This means that the columns "firstname" and "lastname" uniquely identify a professor.

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/5386a42c-bd01-4159-be77-449418d3cf34)

# 10. A professor is uniquely identified by firstname, lastname only

So the "university_shortname" column is not needed in order to reference a professor in the affiliations table. You can remove it, and this will reduce the redundancy in your database again. In other words: The columns "firstname", "lastname", "function", and "organization" are enough to store the affiliation a professor has with a certain organization.

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/428d584a-8d34-4aca-83cf-1051a5a89537)

# 11. Let's get to work!

Time to prepare the database for data migration. After this, you will migrate the data.

# RENAME and DROP COLUMNs in affiliations

As mentioned in the video, the still empty affiliations table has some flaws. In this exercise, you'll correct them as outlined in the video.

You'll use the following queries:

To rename columns:
```
ALTER TABLE table_name
RENAME COLUMN old_name TO new_name;
```

To delete columns:
```
ALTER TABLE table_name
DROP COLUMN column_name;
```

1. Rename the organisation column to organization in affiliations.

'''
-- Rename the organisation column
  ALTER TABLE affiliations
RENAME COLUMN organisation TO organization;
'''

2. Delete the university_shortname column in affiliations.

```
-- Rename the organisation column
  ALTER TABLE affiliations
RENAME COLUMN organisation TO organization;

-- Delete the university_shortname column
ALTER TABLE affiliations
DROP COLUMN university_shortname;
```

Now the tables are finally ready for data migration.

# Migrate data with INSERT INTO SELECT DISTINCT

Now it's finally time to migrate the data into the new tables. You'll use the following pattern:
```
INSERT INTO ... 
SELECT DISTINCT ... 
FROM ...;
```
It can be broken up into two parts:

First part:
```
SELECT DISTINCT column_name1, column_name2, ... 
FROM table_a;
```
This selects all distinct values in table table_a – nothing new for you.

Second part:
```
INSERT INTO table_b ...;
```
Take this part and append it to the first, so it inserts all distinct rows from table_a into table_b.

One last thing: It is important that you run all of the code at the same time once you have filled out the blanks.

```
-- Insert unique professors into the new table
    INSERT INTO professors 
SELECT DISTINCT firstname, lastname, university_shortname 
           FROM university_professors;

-- Doublecheck the contents of professors
SELECT * 
  FROM professors;
```

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/8e44fa0d-aba7-46a8-a062-52ef6918c698)

2. Insert all DISTINCT affiliations into affiliations from university_professors.

```
-- Insert unique affiliations into the new table
    INSERT INTO affiliations 
SELECT DISTINCT firstname, lastname, function, organization 
           FROM university_professors;

-- Doublecheck the contents of affiliations
SELECT * 
  FROM affiliations;
```

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/b825524c-f129-4b88-b82d-bbc5dbbe212e)

You can see that there are 1377 distinct combinations of professors and organisations in the dataset. We'll migrate the other two tables universities and organisations for you. The last thing to do in this chapter is to delete the no longer needed university_professors table.

# Delete tables with DROP TABLE

Obviously, the university_professors table is now no longer needed and can safely be deleted.

For table deletion, you can use the simple command:
```
DROP TABLE table_name;
```

Delete the university_professors table.

```
-- Delete the university_professors table
DROP TABLE university_professors;
```

Now it's finally time to delve into the real advantages of databases. In the following chapters, you will discover many cool features that ultimately lead to better data consistency and quality, such as domain constraints, keys, and referential integrity. See you soon!

