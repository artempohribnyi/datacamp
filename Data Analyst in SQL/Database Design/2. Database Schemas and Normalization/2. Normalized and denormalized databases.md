# 1. Normalized and denormalized databases

Welcome back! Now that we have a grasp on normalization, let's talk about why we would want to normalize a database.

# 2. Back to our book store example

You should be familiar with these two schemas by now. They're both storing fictional company data on the sales of books in bulk to stores across the US and Canada. On the left, you have the star schema with denormalized dimension tables. On the right, you have the snowflake schema with normalized dimension tables. The normalized database looks way more complicated. And it is in some ways. For example, let's say you wanted to get the quantity of all books by Octavia E. Butler sold in Vancouver in Q4 of 2018.

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/8f24404e-4616-4a5b-92a2-a7be6656216a)

# 3. Denormalized query

Based on the denormalized schema, you can run the following query to accomplish this. It's composed of 3 joins, which makes sense based on the three dimension tables in the star schema.

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/6f2389e9-ac11-4293-9c9d-59e6fd5a5ab7)

# 4. Normalized query

What would the query look like on the normalized schema? A lot longer. It doesn't even fit one slide!

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/12600a43-8276-4ad3-9952-d6ea95617731)

# 5. Normalized query (continued)

There's a total of 8 inner joins. This makes sense based on the snowflake schema diagram. The normalized snowflake schema has considerably more tables. This means more joins, which means slower queries. So why would we want to normalize a database?

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/3dbbee55-87e5-4013-a651-f8564711c20f)

# 6. Normalization saves space

Normalization saves space. This isn't intuitive seeing how normalized databases have more tables. Let's take a look at the store table in our denormalized database. Here we see a lot of repeated information in bold - such as USA, California, New York, and Brooklyn. This type of denormalized structure enables a lot of data redundancy.

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/41ca06e6-ad1b-4f42-ba8d-71043a4f7eea)

# 7. Normalization saves space

If we normalize that previous schema, we get this: We see that although we are using more tables, there is no data redundancy. The string, Brooklyn, is only stored once. And the state records are stored separately because many cities share the same state, and country. We don't need to repeat that information, instead, we can have one record holding the string California. Here we see how normalization eliminates data redundancy.

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/c2d4e403-8545-4153-a6c7-ba8218e2755a)

# 8. Normalization ensures better data integrity

Normalization ensures better data integrity through its design. First, it enforces data consistency. Data entry can get messy, and at times people will fill out fields differently. For example, when referring to California, someone might enter the initials "CA". Since the states are already entered in a table, we can ensure naming conventions through referential integrity. Secondly, because duplicates are reduced, modification of any data becomes safer and simpler. Say in the previous example, you wanted to update the spelling of a state - you wouldn't have to find each record referring to the state, instead, you could make that change in the states table by altering one record. From there, you can be confident that the new spelling will be enacted for all stores in that state. Lastly, since tables are smaller and organized more by object, its easier to alter the database schema. You can extend a smaller table without having to alter a larger table holding all the vital data.

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/9371418f-1a7f-46a1-8892-c2c9a39cfa85)

# 9. Database normalization

To recap, here are the pros and cons of normalization. Now normalization seems appealing, especially for database maintenance. However, normalization requires a lot more joins making queries more complicated, which can make indexing and reading of data slower. Deciding between normalization and denormalization comes down to how read- or write- intensive your database is going to be.

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/fa9b99bd-7814-42cb-95b1-c6506b90a1ca)

# 10. Remember OLTP and OLAP?

Remember OLTP and OLAP? Can you guess which prefers normalization? Take a pause and think about it. Did you get it right? OLTP is write-intensive meaning we're updating and writing often. Normalization makes sense because we want to add data quickly and consistently. OLAP is read-intensive because we're running analytics on the data. This means we want to prioritize quicker read queries. Remember how much more joins the normalized query had over the denormalized query? OLAP should avoid that.

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/6af574b5-fe2f-4a57-b407-6e0a93c2accc)

# 11. Let's practice!

Let's see how much you've learned!

# Querying the star schema

The novel genre hasn't been selling as well as your company predicted. To help remedy this, you've been tasked to run some analytics on the novel genre to find which areas the Sales team should target. To begin, you want to look at the total amount of sales made in each state from books in the novel genre.

Luckily, you've just finished setting up a data warehouse with the following star schema:

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/53ad28a7-d3d2-4d92-842d-c06e9ee87fb8)

The tables from this schema have been loaded. Note that you should not use aliases in FROM and JOIN statements.

Select state from the appropriate table and the total sales_amount.
Complete the JOIN on book_id.
Complete the JOIN to connect the dim_store_star table
Conditionally select for books with the genre novel.
Group the results by state.

```
-- Output each state and their total sales_amount
SELECT dim_store_star.state, SUM(fact_booksales.sales_amount)
FROM fact_booksales
	-- Join to get book information
    JOIN dim_book_star ON dim_book_star.book_id = fact_booksales.book_id
	-- Join to get store information
    JOIN dim_store_star ON dim_store_star.store_id = fact_booksales.store_id
-- Get all books with in the novel genre
WHERE  
    dim_book_star.genre = 'novel'
-- Group results by state
GROUP BY
    dim_store_star.state;
```

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/f99de20c-b6fd-4838-a87a-947447cea58a)

We now have a nice list of the amount of money made from novels in each state. Note that it took only two joins to run this query.

# Querying the snowflake schema

Imagine that you didn't have the data warehouse set up. Instead, you'll have to run this query on the company's operational database, which means you'll have to rewrite the previous query with the following snowflake schema:

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/cab13e7e-33b4-4540-a5b7-a0ef35e698b3)

The tables in this schema have been loaded. Remember, our goal is to find the amount of money made from the novel genre in each state.

Select state from the appropriate table and the total sales_amount.
Complete the two JOINS to get the genre_id's.
Complete the three JOINS to get the state_id's.
Conditionally select for books with the genre novel.
Group the results by state.

```
-- Output each state and their total sales_amount
SELECT dim_state_sf.state, SUM(dim_state_sf.sales_amount)
FROM fact_booksales
    -- Joins for genre
    JOIN dim_book_sf on fact_booksales.book_id = dim_book_sf.book_id
    JOIN dim_genre_sf on dim_book_sf.genre_id = dim_genre_sf.genre_id
    -- Joins for state 
    JOIN dim_store_sf on dim_store_sf.store_id = fact_booksales.store_id 
    JOIN dim_city_sf on dim_city_sf.city_id = dim_store_sf.city_id
	JOIN dim_state_sf on  dim_state_sf.state_id = dim_city_sf.state_id
-- Get all books with in the novel genre and group the results by state
WHERE  
    dim_genre_sf.genre = 'novel'
GROUP BY
    dim_state_sf.state;
```

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/18871076-1353-4004-a7cc-f4453a7fcb5a)

This query was definitely more work than the previous one. It wouldn't be practical to have to think about all these joins if you're doing a lot of analytics.

# Updating countries

Going through the company data, you notice there are some inconsistencies in the store addresses. These probably occurred during data entry, where people fill in fields using different naming conventions. This can be especially seen in the country field, and you decide that countries should be represented by their abbreviations. The only countries in the database are Canada and the United States, which should be represented as USA and CA.

In this exercise, you will compare the records that need to be updated in order to do this task on the star and snowflake schema. dim_store_star and dim_country_sf have been loaded.

1. Output all the records that need to be updated in the star schema so that countries are represented by their abbreviations.

```
-- Output records that need to be updated in the star schema
SELECT * FROM dim_store_star
WHERE country != 'USA' AND country !='CA';
```

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/0e579110-bfca-4da6-8edc-a06f5164a347)

2. How many records would need to be updated in the snowflake schema?

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/1f7461e1-da7d-4053-a1ae-c1d6ec083cfd)

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/08d4180b-e920-4991-8f66-3f6b4a1d5912)

Only one record needs to be changed - Canada to CA. Updating is typically simpler in a snowflake schema because there are less records to update because redundant values are minimized to their own table (e.g., countries have their own table, dim_country_sf). Snowflake schemas are also better at enforcing naming conventions due to referential integrity. Note how there weren't any variations in how Canada and USA were referred to in the snowflake schema.

# Extending the snowflake schema

The company is thinking about extending their business beyond bookstores in Canada and the US. Particularly, they want to expand to a new continent. In preparation, you decide a continent field is needed when storing the addresses of stores.

Luckily, you have a snowflake schema in this scenario. As we discussed in the video, the snowflake schema is typically faster to extend while ensuring data consistency. Along with dim_country_sf, a table called dim_continent_sf has been loaded. It contains the only continent currently needed, North America, and a primary key. In this exercise, you'll need to extend dim_country_sf to reference dim_continent_sf.

Add a continent_id column to dim_country_sf with a default value of 1. Note thatNOT NULL DEFAULT(1) constrains a value from being null and defaults its value to 1.
Make that new column a foreign key reference to dim_continent_sf's continent_id.

```
-- Add a continent_id column with default value of 1
ALTER TABLE dim_country_sf
ADD continent_id int NOT NULL DEFAULT(1);

-- Add the foreign key constraint
ALTER TABLE dim_country_sf ADD CONSTRAINT country_continent
   FOREIGN KEY (continent_id) REFERENCES dim_continent_sf(continent_id);
   
-- Output updated table
SELECT * FROM dim_country_sf;
```

![image](https://github.com/artempohribnyi/datacamp/assets/113499718/02884561-d05e-4933-bf11-ebe96d7dfffd)

We have successfully extended the snowflake schema to have continents. That wasn't too bad as it only required altering one table and we can be sure of data consistency. This type of extension is a big benefit of the snowflake schema.

